# -*- coding: utf-8 -*-
"""Copy of Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1J7vQ8PGTHHP9scODnqmqZIXmajC8dzr4
"""

from google.colab import drive

drive.mount('/content/drive')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.image import imread
import cv2
import os
import random
from os import listdir
from PIL import Image
import tensorflow as tf
from keras.preprocessing import image
from tensorflow.keras.utils import img_to_array, array_to_img
from keras.optimizers import Adam
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D
from keras.layers import Activation, Flatten, Dropout, Dense
from sklearn.model_selection import train_test_split
from keras.models import model_from_json
from keras.utils import to_categorical
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from tensorflow.keras.preprocessing import image
from tensorflow.keras.callbacks import EarlyStopping

import tensorflow as tf
from tensorflow.keras import models, layers
import matplotlib.pyplot as plt

IMAGE_SIZE =256
BATCH_SIZE = 32
CHANNELS =3
EPOCHS = 50

dataset=tf.keras.preprocessing.image_dataset_from_directory(
    "/content/drive/MyDrive/model/my models/groundnut raw data set/Groundnut_Leaf_dataset/Groundnut_Leaf_dataset/train",
    shuffle=True,
    image_size=(IMAGE_SIZE,IMAGE_SIZE),
    batch_size= BATCH_SIZE
)

class_names = dataset.class_names
class_names

dict_labels={
    'healthy_leaf_1': 'No deficiency detected',
    'B': 'Boron deficiency',
    'Fe': 'Iron deficiency',
    'K': 'Potassium deficiency',
    'P': 'Phosphorus deficiency',
    'N': 'Nitrogen deficiency'
}
dict_labels

len(dataset)

import os
import pandas as pd
from tabulate import tabulate

data_dir = '/content/drive/MyDrive/model/my models/groundnut raw data set/Groundnut_Leaf_dataset/Groundnut_Leaf_dataset/train'
class_dirs = os.listdir(data_dir)

class_counts = {}

for class_dir in class_dirs:
    files_count = len(os.listdir(os.path.join(data_dir, class_dir)))
    class_counts[class_dir] = files_count

df = pd.DataFrame(list(class_counts.items()), columns=['Class', 'Number of Files'])

print(tabulate(df, headers='keys', tablefmt='psql'))

plt.figure(figsize=(10,10))
for image_batch, label_batch in dataset.take(1):
  for i in range(12):
    ax = plt.subplot(3,4,i+1)
    plt.imshow(image_batch[i].numpy().astype("uint8"))
    plt.title(class_names[label_batch[i]])
    plt.axis("off")

len(dataset)

data_split = [0.8, 0.1, 0.1]

train_size = 0.8
len(dataset)*train_size

train_size = 0.8
len(dataset)*train_size

test_ds=dataset.skip(54)
len(test_ds)

val_size=0.1
len(dataset)*val_size

val_ds=test_ds.take(6)
len(val_ds)

test_ds =test_ds.skip(6)
len(test_ds)

def get_dataset_partitions_tf(ds, train_split=0.1,val_split=0.1, test_split=0.1, shuffle= True, shuffle_size=10000):
  ds_size=len(ds)
  train_size=int(train_split*ds_size)
  val_size=int(val_split*ds_size)

  train_ds =ds.take(train_size)

  val_ds=ds.skip(train_size).take(val_size)
  test_ds=ds.skip(train_size).skip(val_size)

  return train_ds, val_ds, test_ds

train_ds, val_ds, test_ds =get_dataset_partitions_tf(dataset)

len(train_ds)

len(val_ds)

len(test_ds)

train_ds=train_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)
val_ds=val_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)
test_ds=test_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)

import tensorflow as tf
from tensorflow.keras import layers

resize_and_rescale=tf.keras.Sequential([
     layers.Resizing(IMAGE_SIZE, IMAGE_SIZE),  # Use layers.Resizing directly
     layers.Rescaling(1.0/255)
 ])

import tensorflow as tf
from tensorflow.keras import layers

data_segmentaion = tf.keras.Sequential([
    layers.RandomFlip("horizontal_and_vertical"),  # Use layers.RandomFlip directly
    layers.RandomRotation(0.2)  # Use layers.RandomRotation directly
])

import tensorflow as tf
from tensorflow.keras import layers

BATCH_SIZE = 32
IMAGE_SIZE = 128
CHANNELS = 3
input_shape =( BATCH_SIZE,IMAGE_SIZE, IMAGE_SIZE, CHANNELS)
n_classes=6
data_augmentation = tf.keras.Sequential([
    layers.RandomFlip("horizontal_and_vertical"),  # Use layers.RandomFlip directly
    layers.RandomRotation(0.2)  # Use layers.RandomRotation directly
])

model =models.Sequential([
    resize_and_rescale,
    data_augmentation,
    layers.Conv2D(32,(3,3), activation ="relu", input_shape =input_shape),
    layers.MaxPooling2D((2,2)),
    layers.Conv2D(64, kernel_size =(3,3), activation ="relu"),
    layers.MaxPooling2D((2,2)),
    layers.Conv2D(64, kernel_size =(3,3), activation ="relu"),
    layers.MaxPooling2D((2,2)),
    layers.Conv2D(32,(3,3), activation ="relu"),
    layers.MaxPooling2D((2,2)),
    layers.Conv2D(64,(3,3), activation ="relu", padding='same'),
    layers.MaxPooling2D((2,2)),
    layers.Conv2D(32,(3,3), activation ="relu", padding='same'),
    layers.MaxPooling2D((2,2)),
    layers.Flatten(),
     layers.Dense(64, activation ="relu"),
     layers.Dense(n_classes, activation="softmax"),
])
model.build(input_shape=input_shape)

model.summary()

model.compile(
    optimizer="adam",
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
    metrics=["accuracy"]
)

early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

history=model.fit(
    train_ds,
    epochs=EPOCHS,
    batch_size=BATCH_SIZE,
    verbose=1,
    validation_data=val_ds)

print(test_ds)

scores = model.evaluate(test_ds)
print(scores)

scores

history

history.params

history.history.keys()

history.history["accuracy"]

acc=history.history["accuracy"]
val_acc= history.history["val_accuracy"]

loss= history.history["loss"]
val_loss= history.history["val_loss"]

plt.figure(figsize=(7,5))
plt.subplot(1,2,1)
plt.plot(range(EPOCHS), acc, label="Traning Accuracy")
plt.plot(range(EPOCHS), val_acc, label="Validation Accuracy")
plt.legend(loc="lower right")
plt.title("Traning and Validation Accuracy")


plt.subplot(1,2,2)
plt.plot(range(EPOCHS), loss, label="Traning loss")
plt.plot(range(EPOCHS), val_loss, label="Validation loss")
plt.legend(loc="upper right")
plt.title("Traning and Validation loss")
plt.show()

np.argmax([8.0800897e-01 ,1.9199099e-01 ,4.3657113e-09])

import numpy as np
for images_batch, labels_batch in test_ds.take(1):
  first_image=images_batch[0].numpy().astype("uint8")
  first_label= labels_batch[0]

  print("first image to predict")
  plt.imshow(first_image)
  print("first image's actual label:", class_names[first_label])

  batch_prediction = model.predict(images_batch)
  print("predicted deficiency :",class_names[np.argmax(batch_prediction[0])])

def predict(model, img):
  img_array= tf.keras.preprocessing.image.img_to_array(images[i].numpy())
  img_array = tf.expand_dims(img_array, 0)

  predictions = model.predict(img_array)

  predicted_class = class_names[np.argmax(predictions[0])]
  confidence= round(100*(np.max(predictions[0])), 2)
  return predicted_class, confidence

plt.figure(figsize=(15,15))

for images, labels in test_ds.take(1):
  for i in range(10):
    ax =plt.subplot(3,4,i+1)
    plt.imshow(images[i].numpy().astype("uint8"))

    predicted_class, confidence = predict(model, images[i].numpy())
    actual_class = class_names[labels[i]]

    plt.title(f"Actual: {actual_class},\n Predicted: {predicted_class}.\n Confidence: {confidence},\n Deficiency:{dict_labels[predicted_class]}")
    plt.axis("off")

y_true = np.concatenate([y for x, y in test_ds], axis=0)

y_pred_prob = model.predict(test_ds)
y_pred_classes = np.argmax(y_pred_prob, axis=1)

print("True labels:", y_true)
print("Predicted labels:", y_pred_classes)

y_true = np.concatenate([y for x, y in test_ds], axis=0)

from sklearn.metrics import classification_report

print(classification_report(y_true, y_pred_classes, target_names=['Healthy_leaf_1', 'B', 'Fe', 'K', 'P', 'N']))

print(y_true, y_pred_classes)

import seaborn as sns
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
conf_matrix = confusion_matrix(y_true, y_pred_classes)

plt.figure(figsize=(7, 5))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',xticklabels=['Healthy_leaf_1', 'B', 'Fe', 'K', 'P', 'N'],yticklabels=['Healthy_leaf_1', 'B', 'Fe', 'K', 'P', 'N'])
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

import os
os.listdir("/content/drive/MyDrive/model/my models/groundnut raw data set/Groundnut_Leaf_dataset/Groundnut_Leaf_dataset/train")

model_path = '/content/drive/MyDrive/model/my models/model.h5'
# Save the model
model.save(model_path)

print(f"Model saved to {model_path}")

saved_model_path = '/content/drive/MyDrive/model/my models/model.h5'

# Load the saved model
loaded_model = tf.keras.models.load_model(saved_model_path)

# Example usage of loaded model for prediction
# loaded_model.predict(...)

print(f"Model loaded from {saved_model_path}")

converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

len(tflite_model)

converter =  tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_quant_model = converter.convert()

len(tflite_quant_model)

with open("tflite_model.tflite", "wb") as f:
  f.write(tflite_model)

with open("tflite_model.tflite", "wb") as f:
  f.write(tflite_quant_model)

import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Input

# Example model architecture
inputs = Input(shape=(784,))
x = Dense(128, activation='relu')(inputs)
outputs = Dense(10, activation='softmax')(x)
model = Model(inputs=inputs, outputs=outputs)

# Compile and train the model (example)
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
# model.fit(...)

# Specify the path to save the quantized model
quantized_model_path = '/content/drive/MyDrive/model/my models/quantized_model'

# Convert the model to a quantized model
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]  # Quantize weights to 8-bit integers
quantized_tflite_model = converter.convert()

# Save the quantized model to disk
with open(quantized_model_path, 'wb') as f:
    f.write(quantized_tflite_model)

print(f"Quantized model saved to {quantized_model_path}")